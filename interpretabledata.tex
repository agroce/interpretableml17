\documentclass[final]{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\DeclareMathOperator{\Update}{update}

\title{Provenance and Pseudo-Provenance for Seeded Automated Test Generation}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
Alex Groce\\
School of Informatics, Computing, and Cyber Systems\\
Northern Arizona University\\
Flagstaff, AZ 86011
\And
Josie Holmes\\
Pennsylvania State University 
}


%\author{
 % David S.~Hippocampus\thanks{Use footnote for providing further
  %  information about author (webpage, alternative
   % address)---\emph{not} for acknowledging funding agencies.} \\
  %Department of Computer Science\\
  %Cranberry-Lemon University\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
%}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Many methods for automated test generation, including some that
  explicitly use machine learning (and some that use ML more broadly
  conceived) derive new tests from existing tests (often referred to
  as seeds).  Often, the seed tests
  from which new tests are derived are manually constructed, or at least
  simpler than the tests that are produced as the final outputs of
  such test generators.  We
  propose annotation of generated tests with a \emph{provenance} (trail) showing
  how individual generated tests of interest (especially failing tests) derive
  from seed tests, and how the population of generated tests relates
  to the original seed tests.  In some cases, post-processing of
  generated tests can invalidate provenance information, in which case
  we also propose a method for attempting to construct
  ``pseudo-provenance'' describing how the tests \emph{could} have
  been (partly) generated from seeds.
\end{abstract}

\section{Seeded Automated Test Generation}

Automatic generation of software tests, including (security) fuzzing
\cite{aflfuzz,TrailBitsSeeded}, random testing \cite{csmith,ICSEDiff,Pacheco},
search-based/evolutionary testing \cite{FA11},
and symbolic or concolic execution
\cite{Whitebox,GodefroidKS05,KLEE,Person:2011:DIS:1993498.1993558,Marinescu:2012:MTS:2337223.2337308,issta14}
is essential for improving software security and reliability.  Many of
these techniques rely on some form of learning, sometimes directly
using standard algorithms, and sometimes in a more broadly conceived
way.  In fact, using Mitchell's classic definition of machine learning
as concerning any computer program that improves its performance at some
task through experience \cite{Mitchell}, almost all non-trivial automated test generation
algorithms are machine-learning systems, with the following
approximate description:

\begin{enumerate}
\item Based on results of running all past tests ($T$), produce a new test
  $t' = f(T)$ to execute.
\item Execute $t'$ and collect data $d'$ on code coverage, fault detection and other
  information of interest for the execution of $t'$.
\item $T = \Update(T,t',d')$
\item Go to step 1.
\end{enumerate}

Performance here (in Mitchell's sense) is usually measured by
collective code coverage or fault detection of tests in $T$, or may be
defined over only a subset of the tests (those deemed most useful,
output as a test suite).  The function $f$ varies widely: $f$ may
represent random testing with probabilities of actions determined by
past executions \cite{AndrewsL07}, a genetic-algorithms approach where
tests in $T$ are mutated and/or combined with each other, based on
their individual performances
\cite{McMinn04search-basedsoftware,FA11,aflfuzz}, or an approach using
symbolic execution to discover new tests satisfying certain
constraints on the execution path \cite{Whitebox,GodefroidKS05,KLEE}.
A common feature however, is that many methods do not begin with the
``blank slate'' of an empty $T$.  Instead, they take as an initial
input a population of tests that are thought to be high-quality (and,
most importantly, to provide some guidance as to the structure of
valid tests), and proceed to generate new tests from these \emph{seed}
tests
\cite{aflfuzz,Person:2011:DIS:1993498.1993558,Marinescu:2012:MTS:2337223.2337308,issta14,STVR_seeding}.
Seed tests are usually manually generated, or tests selected from a
previously generated suite for their high coverage or fault detection
\cite{YooHarman,stvrcausereduce}.  It is generally the case that seed
tests are more easily understood by users than newly generated tests.

For example, consider the extremely popular and successful American
Fuzzy Lop (AFL) tool
for security fuzzing \cite{aflfuzz}.  It usually begins fuzzing
(trying to generate inputs that cause crashes indicating potential
security vulnerabilities) from a corpus of ``good'' inputs to a
program.  When a corpus input is mutated and the result is
``interesting,'' by a code-coverage based heuristic, the new input is
added to the corpus of tests to use in creating future tests.  Many
tools, considered at a high level, operate in the same fashion, with the
critical differences arising from engineering aspects (how tests are executed
and instrumented), varied heuristics for selecting tests to mutate,
and choice of mutation methods.  AFL records the origin of each test
in its queue in test filenames, which suffices in AFL's case because
each test produced is the result of a change to a single, pre-existing
test, in most cases, or the merger of two tests, in rarer cases.

This kind of trace back to the source of a generated test in some seed
test (possibly through a long trail of also-generated tests) is
essentially a \emph{provenance}, which we argue is the most easily
understood explanation of a learning result for humans, in those cases
(such as testing) where the algorithm's purpose is to produce
novel, interesting objects from existing objects.

This simple approach used in AFL works for cases where the provenance of a test is
always mediated by mutation, making for clear, simple ``audit trail.''  However, a more complex approach is
required when the influence of seeds is probabilistic, or a test is
composed of (partial) fragments of many tests.  Moreover, AFL provides
no tools to guide users in making use of what amounts to an internal
book-keeping mechanism, rather than an output designed for human
examination.
Finally, tests, once generated, are frequently manipulated in ways
that may make provenance information no longer valid:  a test produced
from two seed tests (or seed test-derived tests) may be reduced
\cite{DD} so that one of the tests no longer is present at all, for example.

In this paper, we propose to go beyond the kind of simple mechanisms
found in AFL, and
offer the following contributions:

\begin{itemize}
\item We present an implementation of provenance for an algorithm that involves
  generating new tests from partial sequences from many seed tests.
\item We discuss ways to present information about not just the
  provenance of a single test, but the impact on future tests of
  initial seed tests.  While single-test provenance is useful for
  developers debugging, information on general impact of seeds is more
  important for design and analysis of test generation configuration
  and algorithms.
\item We identify test manipulations that partially or completely
  destroy/invalidate provenance information, and propose an algorithm
  for producing a \emph{pseudo-provenance}, showing how the tests generated
  \emph{could} have been generated from seeds, even if they were not actually
  thus generated, and discuss abstractions that allow the
  creation of such a psuedo-provenances in more cases.
\end{itemize}

\section{A Simple Seeded Generation Algorithm with Provenance}

We implemented a novel test generation technique for the TSTL
\cite{tstlsttt,nfm15,issta15} test generation language and tool for
Python.  In this approach, the seed tests are split into short
sub-sequences of length $k$.  In place of the usual algorithm for
random testing, where a new test action is randomly chosen at each
step during testing, our approach always attempts to follow some
sub-sequence, in a best-effort fashion (if the next step in the
current sub-sequence is not enabled, it is skipped).  When a test
generated in this fashion covers new code (the usual metric for
deciding to learn from a test), it too is broken into sub-sequences
and they are added to the sub-sequence pool.

\section{Presenting Collective Provenance Information}

\section{Test Manipulations and Pseudo-Provenance}

\subsection{A Greedy Pseudo-Provenance Algorithm}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

%\section*{References}

\bibliographystyle{plain}
\bibliography{intml}

\end{document}
